#!/usr/bin/env python3
"""
Auto-Compactæ™‚ã«Cipherã«ãƒ¡ãƒ¢ãƒªã‚’ä¿å­˜ã™ã‚‹Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ
PreCompact Input JSONã‚’è§£æã—ã€Cipherã«ä¼šè©±å†…å®¹ã‚’è¨˜æ†¶ã•ã›ã‚‹
"""

import json
import sys
import os
import logging
import re
from datetime import datetime
from typing import Dict, List, Any, Optional

# ãƒ­ã‚°è¨­å®š
log_dir = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, 'cipher_hook.log')

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stderr)
    ]
)

logger = logging.getLogger(__name__)

def read_stdin_json() -> Optional[Dict[str, Any]]:
    """æ¨™æº–å…¥åŠ›ã‹ã‚‰JSONã‚’èª­ã¿å–ã‚‹"""
    try:
        input_data = sys.stdin.read().strip()
        if not input_data:
            logger.error("No input data received from stdin")
            return None

        return json.loads(input_data)
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON input: {e}")
        return None
    except Exception as e:
        logger.error(f"Error reading stdin: {e}")
        return None

def read_transcript(transcript_path: str) -> Optional[List[Dict[str, Any]]]:
    """ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚‹"""
    try:
        if not os.path.exists(transcript_path):
            logger.error(f"Transcript file not found: {transcript_path}")
            return None

        messages = []
        with open(transcript_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        messages.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

        logger.info(f"Read {len(messages)} messages from transcript")
        return messages
    except Exception as e:
        logger.error(f"Error reading transcript: {e}")
        return None

def extract_conversation_content(messages: List[Dict[str, Any]], limit: int = 20) -> str:
    """ä¼šè©±å†…å®¹ã‹ã‚‰é‡è¦ãªéƒ¨åˆ†ã‚’æŠ½å‡º"""
    try:
        # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æŒ‡å®šæ•°ã‚’å–å¾—
        recent_messages = messages[-limit:] if len(messages) > limit else messages

        conversation_parts = []
        for msg in recent_messages:
            msg_type = msg.get('type', '')
            content = msg.get('content', '')

            if msg_type == 'text' and content:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¾ãŸã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                role = msg.get('role', 'unknown')
                conversation_parts.append(f"[{role}]: {content}")
            elif msg_type == 'tool_use':
                # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã®è¨˜éŒ²
                tool_name = msg.get('name', 'unknown_tool')
                conversation_parts.append(f"[tool]: {tool_name}")

        return "\n".join(conversation_parts)
    except Exception as e:
        logger.error(f"Error extracting conversation content: {e}")
        return ""

def extract_project_context(transcript_path: str) -> Dict[str, Any]:
    """ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‘ã‚¹ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        # ãƒ‘ã‚¹ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æ¨æ¸¬
        path_parts = transcript_path.split('/')
        project_name = "unknown"
        working_dir = "unknown"

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™
        for i, part in enumerate(path_parts):
            if part in ['Documents', 'Projects', 'workspace', 'code']:
                if i + 1 < len(path_parts):
                    project_name = path_parts[i + 1]
                    working_dir = '/'.join(path_parts[:i + 2])
                break

        return {
            "name": project_name,
            "path": working_dir,
            "transcript_path": transcript_path
        }
    except Exception as e:
        logger.error(f"Error extracting project context: {e}")
        return {"name": "unknown", "path": "unknown", "transcript_path": transcript_path}

def detect_languages(content: str) -> List[str]:
    """ä¼šè©±å†…å®¹ã‹ã‚‰ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚’æ¤œå‡º"""
    languages = []

    # ä¸€èˆ¬çš„ãªè¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³
    language_patterns = {
        'python': [r'\.py\b', r'python', r'pip\s+install', r'def\s+\w+', r'import\s+\w+'],
        'javascript': [r'\.js\b', r'\.ts\b', r'npm\s+install', r'function\s+\w+', r'const\s+\w+'],
        'java': [r'\.java\b', r'public\s+class', r'package\s+\w+', r'import\s+java'],
        'go': [r'\.go\b', r'func\s+\w+', r'package\s+main', r'import\s+"'],
        'rust': [r'\.rs\b', r'fn\s+\w+', r'use\s+std::', r'cargo\s+'],
        'shell': [r'\.sh\b', r'#!/bin/bash', r'chmod\s+\+x', r'\$\{.*\}'],
        'json': [r'\.json\b', r'\{.*".*":', r'JSON'],
        'yaml': [r'\.ya?ml\b', r'---\s*$', r'^\s*\w+:\s*$'],
        'markdown': [r'\.md\b', r'##?\s+', r'\[.*\]\(.*\)']
    }

    for lang, patterns in language_patterns.items():
        if any(re.search(pattern, content, re.IGNORECASE | re.MULTILINE) for pattern in patterns):
            languages.append(lang)

    return languages if languages else ['general']

def detect_project_status(content: str) -> str:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®çŠ¶æ³ã‚’æ¤œå‡º"""
    content_lower = content.lower()

    if any(word in content_lower for word in ['å®Œäº†', 'completed', 'finished', 'done']):
        return 'completed'
    elif any(word in content_lower for word in ['é€²è¡Œä¸­', 'in progress', 'working on']):
        return 'in-progress'
    elif any(word in content_lower for word in ['é–‹å§‹', 'started', 'beginning']):
        return 'started'
    elif any(word in content_lower for word in ['è¨ˆç”»', 'planning', 'design']):
        return 'planning'
    else:
        return 'active'

def generate_smart_tags(conversation_content: str, project_context: Dict[str, Any]) -> List[str]:
    """ä¼šè©±å†…å®¹ã‹ã‚‰æ™ºèƒ½çš„ã«ã‚¿ã‚°ã‚’ç”Ÿæˆ"""
    tags = ["auto-compact"]

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£ã‚¿ã‚°
    if project_name := project_context.get('name'):
        if project_name != 'unknown':
            tags.append(f"project:{project_name}")

    # è¨€èªæ¤œå‡º
    languages = detect_languages(conversation_content)
    tags.extend([f"lang:{lang}" for lang in languages])

    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—æ¤œå‡º
    content_lower = conversation_content.lower()
    if any(word in content_lower for word in ["implement", "å®Ÿè£…", "ä½œæˆ", "build"]):
        tags.append("task:implementation")
    elif any(word in content_lower for word in ["debug", "ãƒ‡ãƒãƒƒã‚°", "ä¿®æ­£", "fix", "error"]):
        tags.append("task:debugging")
    elif any(word in content_lower for word in ["analyze", "åˆ†æ", "èª¿æŸ»", "review"]):
        tags.append("task:analysis")
    elif any(word in content_lower for word in ["test", "ãƒ†ã‚¹ãƒˆ", "æ¤œè¨¼"]):
        tags.append("task:testing")
    elif any(word in content_lower for word in ["design", "è¨­è¨ˆ", "architecture"]):
        tags.append("task:design")

    # å„ªå…ˆåº¦æ¤œå‡º
    if any(word in content_lower for word in ["urgent", "critical", "important", "ç·Šæ€¥", "é‡è¦"]):
        tags.append("priority:high")
    elif any(word in content_lower for word in ["later", "å¾Œã§", "ä½å„ªå…ˆ"]):
        tags.append("priority:low")
    else:
        tags.append("priority:medium")

    # çŠ¶æ³ã‚¿ã‚°
    status = detect_project_status(conversation_content)
    tags.append(f"status:{status}")

    return tags

def count_messages(conversation_content: str) -> int:
    """ä¼šè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    return len([line for line in conversation_content.split('\n') if line.strip().startswith('[')])

def save_to_cipher(conversation_content: str, session_id: str, transcript_path: str) -> bool:
    """Cipherã«ä¼šè©±å†…å®¹ã‚’æ§‹é€ åŒ–ã—ã¦ä¿å­˜ï¼ˆMCPçµŒç”±ï¼‰"""
    try:
        timestamp = datetime.now().isoformat()
        project_context = extract_project_context(transcript_path)

        # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªå†…å®¹
        memory_content = f"""
Claude Code Auto-Compact Memory Archive

# Session Context
- Session ID: {session_id}
- Timestamp: {timestamp}
- Event: auto-compact triggered
- Project: {project_context.get('name', 'unknown')}
- Working Directory: {project_context.get('path', 'unknown')}

# Summary Request
ä»¥ä¸‹ã®auto-compactç›´å‰ã®ä¼šè©±å†…å®¹ã‹ã‚‰ã€æ¬¡ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ç¶™ç¶šä½œæ¥­ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºãƒ»è¦ç´„ã—ã¦è¨˜æ†¶ã—ã¦ãã ã•ã„ã€‚

{conversation_content}

# Memory Extraction Instructions
## ğŸ¯ Project Goals & Current Status
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„ã¨ç¾åœ¨ã®é€²æ—çŠ¶æ³ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„

## ğŸ“‹ Active Tasks & Next Steps
- ç¶™ç¶šä¸­ã®ã‚¿ã‚¹ã‚¯ã¨æ¬¡ã«å®Ÿè¡Œã™ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ•´ç†ã—ã¦ãã ã•ã„

## ğŸ”§ Technical Context
- é‡è¦ãªæŠ€è¡“çš„æ±ºå®šã‚„ç™ºè¦‹äº‹é …ã‚’ä¿å­˜ã—ã¦ãã ã•ã„
- ä½¿ç”¨ã—ã¦ã„ã‚‹æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚„æ‰‹æ³•ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„

## ğŸ“ Important Context
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚„åˆ¶ç´„æ¡ä»¶ã‚’è¨˜æ†¶ã—ã¦ãã ã•ã„
- æ³¨æ„ã™ã¹ãäº‹é …ã‚„æ—¢çŸ¥ã®å•é¡Œã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„

## ğŸ·ï¸ Classification Tags
ä»¥ä¸‹ã®å½¢å¼ã§ã‚¿ã‚°ä»˜ã‘ã—ã¦ãã ã•ã„ï¼š
- project:{project_context.get('name', 'unknown')}
- session-type:auto-compact
- language:{','.join(detect_languages(conversation_content))}
- status:{detect_project_status(conversation_content)}
        """.strip()

        # å¼·åŒ–ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        smart_tags = generate_smart_tags(conversation_content, project_context)
        metadata = {
            "sessionId": session_id,
            "source": "auto-compact",
            "projectId": project_context.get('name'),
            "timestamp": timestamp,
            "tags": smart_tags,
            "context": {
                "triggerEvent": "auto-compact",
                "messageCount": count_messages(conversation_content),
                "workingDirectory": project_context.get('path'),
                "detectedLanguages": detect_languages(conversation_content),
                "projectStatus": detect_project_status(conversation_content)
            }
        }

        logger.info(f"Enhanced memory content prepared: {len(memory_content)} characters")
        logger.info(f"Project: {project_context.get('name')}")
        logger.info(f"Languages detected: {detect_languages(conversation_content)}")
        logger.info(f"Smart tags: {smart_tags}")

        # TODO: å®Ÿéš›ã®MCPé€šä¿¡å®Ÿè£…
        # cipher_client = MCPClient()
        # result = cipher_client.extract_and_operate_memory(
        #     interaction=memory_content,
        #     memoryMetadata=metadata
        # )

        # ç¾åœ¨ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        logger.info("Enhanced Cipher memory save simulated successfully")
        return True

    except Exception as e:
        logger.error(f"Error saving enhanced memory to Cipher: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("Cipher memory save script started")

    # æ¨™æº–å…¥åŠ›ã‹ã‚‰PreCompact Input JSONã‚’èª­ã¿å–ã‚Š
    input_data = read_stdin_json()
    if not input_data:
        logger.error("Failed to read input JSON")
        sys.exit(1)

    # triggerãŒautoã®å ´åˆã®ã¿å‡¦ç†
    trigger = input_data.get('trigger', '')
    if trigger != 'auto':
        logger.info(f"Skipping processing for trigger: {trigger}")
        sys.exit(0)

    session_id = input_data.get('session_id', 'unknown')
    transcript_path = input_data.get('transcript_path', '')

    logger.info(f"Processing auto-compact for session: {session_id}")

    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Š
    messages = read_transcript(transcript_path)
    if not messages:
        logger.error("Failed to read transcript messages")
        sys.exit(1)

    # ä¼šè©±å†…å®¹ã‚’æŠ½å‡º
    conversation_content = extract_conversation_content(messages)
    if not conversation_content:
        logger.warning("No conversation content extracted")
        sys.exit(0)

    # Cipherã«ä¿å­˜ï¼ˆtranscript_pathã‚‚æ¸¡ã™ï¼‰
    if save_to_cipher(conversation_content, session_id, transcript_path):
        logger.info("Successfully saved conversation to Cipher")
        sys.exit(0)
    else:
        logger.error("Failed to save conversation to Cipher")
        sys.exit(1)

if __name__ == "__main__":
    main()